{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "machine_translation2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFAll4F8m1hM"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import numpy as np\n",
        "import time"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMiH5C6npt3"
      },
      "source": [
        " ### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jFkWcU1nlk2"
      },
      "source": [
        "file = open(\"swe.txt\", 'r')\n",
        "lines = file.readlines()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN0u4mrqnypT"
      },
      "source": [
        "eng_swe = np.array([line.split(\"\\t\")[0:2] for line in lines])\n",
        "english = eng_swe[:, 0]\n",
        "swedish = eng_swe[:, 1]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKev8aZpn1er"
      },
      "source": [
        "### Filter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdORDbBMnzt0"
      },
      "source": [
        "def punct(elt):\n",
        "  elts = re.sub(\"(?<=.)!\", \" !\", elt)\n",
        "  elts = re.sub(\"(?<=.)\\?\", \" ?\", elts)\n",
        "  elts = re.sub(\"(?<=.)\\.\", \" .\", elts)\n",
        "  elts = re.sub(\"(?<=.),\", \" ,\", elts)\n",
        "  elts = re.sub(\"(?<=.);\", \" ;\", elts)\n",
        "  return elts\n",
        "\n",
        "def start_end(elt):\n",
        "  return \"<start> \" + elt + \" <end>\""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVuNb6YDn7qm"
      },
      "source": [
        "english_cleaned = [start_end(punct(elt)) for elt in english]\n",
        "swedish_cleaned = [start_end(punct(elt)) for elt in swedish]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtgNkANrn_o-"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_TdMGMsn9pX"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CweHDLlioBDW"
      },
      "source": [
        "english_token = Tokenizer(filters='')\n",
        "english_token.fit_on_texts(english_cleaned)\n",
        "\n",
        "swedish_token = Tokenizer(filters='')\n",
        "swedish_token.fit_on_texts(swedish_cleaned)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrb-BajoCGu"
      },
      "source": [
        "english_sentences = english_token.texts_to_sequences(english_cleaned)\n",
        "swedish_sentences = swedish_token.texts_to_sequences(swedish_cleaned)\n",
        "\n",
        "english_sentences = english_sentences[0:15000]\n",
        "swedish_sentences = swedish_sentences[0:15000]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWDoIlijoDEk"
      },
      "source": [
        "english_sentences = pad_sequences(english_sentences, padding='post')\n",
        "swedish_sentences = pad_sequences(swedish_sentences, padding='post')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS-I20ekoFx6"
      },
      "source": [
        "### Tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1wQ8A9ioEIk"
      },
      "source": [
        "BUFFER_SIZE = len(english_sentences)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(english_sentences)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_english = len(english_token.word_index)+1\n",
        "vocab_swedish = len(swedish_token.word_index)+1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_sentences, swedish_sentences)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMX0N6zXojNS"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEztph6IokEh"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uh7DcN2okd5"
      },
      "source": [
        "class Encoder(Model):\n",
        "\n",
        "  def __init__(self, units, vocab_size, embedding_dim):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, output_dim=embedding_dim)\n",
        "    self.lstm = LSTM(units, return_state=True, return_sequences=True)                  \n",
        "                     \n",
        "  def call(self, data, hidden):\n",
        "    \n",
        "    w = self.embedding(data)\n",
        "    output, state_h, state_c = self.lstm(w, initial_state = hidden)\n",
        "\n",
        "    return output, state_h, state_c"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smfB0co8omCP"
      },
      "source": [
        "encoder = Encoder(units, vocab_english, embedding_dim)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLMA8coWonGy"
      },
      "source": [
        "class Attention(Layer):\n",
        "  def __init__(self, units):\n",
        "    super(Attention, self).__init__()\n",
        "    self.dense_key = Dense(units)\n",
        "    self.dense_query = Dense(units)\n",
        "    self.dense_end = Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "\n",
        "    query_encoded = self.dense_query(tf.expand_dims(query, 1))\n",
        "    key_encoded = self.dense_key(values)\n",
        "\n",
        "    final_scores = self.dense_end(\n",
        "        query_encoded * key_encoded)\n",
        "    \n",
        "    attention_weights = tf.nn.softmax(final_scores, axis=1)\n",
        "\n",
        "    filtered_context = attention_weights * values\n",
        "    filtered_context = tf.reduce_sum(filtered_context, axis=1)\n",
        "\n",
        "    return filtered_context, attention_weights"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvE18ciOoofC"
      },
      "source": [
        "class Decoder(Model):\n",
        "  def __init__(self, units, vocab_size ,embedding_dim):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.embedding = Embedding(vocab_size, embedding_dim)\n",
        "    self.lstm = LSTM(units, return_sequences=True, return_state=True)\n",
        "    self.output_layer = Dense(vocab_size)\n",
        "\n",
        "    self.attention = Attention(30)\n",
        "\n",
        "  def call(self, data, hidden, encoder_output):\n",
        "\n",
        "    embedd = self.embedding(data)\n",
        "\n",
        "    context, _ = self.attention(hidden, encoder_output)\n",
        "    context = tf.expand_dims(context, axis=1)\n",
        "\n",
        "    all_data = tf.concat([context, embedd], axis=-1)\n",
        "    output, state_h, state_c = self.lstm(all_data)\n",
        "\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    pred = self.output_layer(output)    \n",
        "  \n",
        "    return pred, state_h, state_c"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHGXfwUOopzT"
      },
      "source": [
        "decoder = Decoder(units, vocab_swedish, embedding_dim)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5Ej3kt0osK4"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SaVM400oq6a"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv9U73MyotJl"
      },
      "source": [
        "def train_step(inp, target):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    hiddens_init = [tf.zeros((BATCH_SIZE , units)), tf.zeros((BATCH_SIZE , units))]\n",
        "    output, dec_hidden_h, dec_hidden_c = encoder(inp, hiddens_init)\n",
        "\n",
        "    decoder_first_input = tf.expand_dims([swedish_token.word_index['<start>']] * 64, 1)\n",
        "    dec_input = decoder_first_input\n",
        "\n",
        "    for i in range(1, target.shape[1]):\n",
        "      predictions, dec_hidden_h, dec_hidden_c = decoder(dec_input, tf.concat([dec_hidden_h, dec_hidden_c], axis=-1), output)\n",
        "      loss += loss_function(target[:, i], predictions)\n",
        "      \n",
        "      dec_input = tf.reshape(target[:, i], (64, 1))\n",
        "\n",
        "  batch_loss = (loss / int(target.shape[1]))\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTUVzOYGouHP",
        "outputId": "6553f3ab-c25f-44b7-f978-2acc4280dd70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch in range(15):\n",
        "  start = time.time()\n",
        "  print(\"Epoch : {}\".format(epoch))\n",
        "  for (batch, (inp, target)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    v = train_step(inp, target)\n",
        "    if batch % 100 == 0:\n",
        "      print(\"\\tBatch {}, Error : {}\".format(batch,v))\n",
        "  print(\"\\tTemps pour 1 epoch {} sec\\n\".format(time.time() - start))           "
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0\n",
            "\tBatch 0, Error : 4.105288982391357\n",
            "\tBatch 100, Error : 2.054779291152954\n",
            "\tBatch 200, Error : 1.9581010341644287\n",
            "\tTemps pour 1 epoch 60.152321338653564 sec\n",
            "\n",
            "Epoch : 1\n",
            "\tBatch 0, Error : 1.969620943069458\n",
            "\tBatch 100, Error : 1.768649697303772\n",
            "\tBatch 200, Error : 1.4792749881744385\n",
            "\tTemps pour 1 epoch 55.69831037521362 sec\n",
            "\n",
            "Epoch : 2\n",
            "\tBatch 0, Error : 1.5808970928192139\n",
            "\tBatch 100, Error : 1.5116809606552124\n",
            "\tBatch 200, Error : 1.4190105199813843\n",
            "\tTemps pour 1 epoch 56.16856145858765 sec\n",
            "\n",
            "Epoch : 3\n",
            "\tBatch 0, Error : 1.339540719985962\n",
            "\tBatch 100, Error : 1.1547611951828003\n",
            "\tBatch 200, Error : 1.3302462100982666\n",
            "\tTemps pour 1 epoch 55.84640169143677 sec\n",
            "\n",
            "Epoch : 4\n",
            "\tBatch 0, Error : 1.1450202465057373\n",
            "\tBatch 100, Error : 0.9964696168899536\n",
            "\tBatch 200, Error : 1.2382603883743286\n",
            "\tTemps pour 1 epoch 55.94716191291809 sec\n",
            "\n",
            "Epoch : 5\n",
            "\tBatch 0, Error : 0.9626413583755493\n",
            "\tBatch 100, Error : 0.9303821921348572\n",
            "\tBatch 200, Error : 0.9708364605903625\n",
            "\tTemps pour 1 epoch 56.116392612457275 sec\n",
            "\n",
            "Epoch : 6\n",
            "\tBatch 0, Error : 0.8219376802444458\n",
            "\tBatch 100, Error : 0.7634758353233337\n",
            "\tBatch 200, Error : 0.7852982878684998\n",
            "\tTemps pour 1 epoch 54.909099102020264 sec\n",
            "\n",
            "Epoch : 7\n",
            "\tBatch 0, Error : 0.6572772264480591\n",
            "\tBatch 100, Error : 0.6042779088020325\n",
            "\tBatch 200, Error : 0.6258984804153442\n",
            "\tTemps pour 1 epoch 54.846646785736084 sec\n",
            "\n",
            "Epoch : 8\n",
            "\tBatch 0, Error : 0.5358948111534119\n",
            "\tBatch 100, Error : 0.6266773343086243\n",
            "\tBatch 200, Error : 0.6370928883552551\n",
            "\tTemps pour 1 epoch 55.137582778930664 sec\n",
            "\n",
            "Epoch : 9\n",
            "\tBatch 0, Error : 0.4406443238258362\n",
            "\tBatch 100, Error : 0.40666601061820984\n",
            "\tBatch 200, Error : 0.5561040043830872\n",
            "\tTemps pour 1 epoch 54.98985552787781 sec\n",
            "\n",
            "Epoch : 10\n",
            "\tBatch 0, Error : 0.3127642273902893\n",
            "\tBatch 100, Error : 0.3872094750404358\n",
            "\tBatch 200, Error : 0.3924708664417267\n",
            "\tTemps pour 1 epoch 54.15263295173645 sec\n",
            "\n",
            "Epoch : 11\n",
            "\tBatch 0, Error : 0.27635252475738525\n",
            "\tBatch 100, Error : 0.3292122781276703\n",
            "\tBatch 200, Error : 0.36713165044784546\n",
            "\tTemps pour 1 epoch 52.189035415649414 sec\n",
            "\n",
            "Epoch : 12\n",
            "\tBatch 0, Error : 0.2346961945295334\n",
            "\tBatch 100, Error : 0.28004705905914307\n",
            "\tBatch 200, Error : 0.31088683009147644\n",
            "\tTemps pour 1 epoch 52.29650378227234 sec\n",
            "\n",
            "Epoch : 13\n",
            "\tBatch 0, Error : 0.18751618266105652\n",
            "\tBatch 100, Error : 0.25713875889778137\n",
            "\tBatch 200, Error : 0.27434393763542175\n",
            "\tTemps pour 1 epoch 51.256616830825806 sec\n",
            "\n",
            "Epoch : 14\n",
            "\tBatch 0, Error : 0.19394908845424652\n",
            "\tBatch 100, Error : 0.17958533763885498\n",
            "\tBatch 200, Error : 0.2694306969642639\n",
            "\tTemps pour 1 epoch 50.99320602416992 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS2qKhS28QIE"
      },
      "source": [
        ""
      ],
      "execution_count": 19,
      "outputs": []
    }
  ]
}