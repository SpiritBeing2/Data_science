{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "transformer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYlefoulVCj7"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.ticker as ticker\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "import re\r\n",
        "import numpy as np\r\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdVZmC5LXJ2v"
      },
      "source": [
        "### Read data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gadDxohnVns8"
      },
      "source": [
        "file = open(\"drive/MyDrive/data/fra.txt\", 'r')\r\n",
        "lines = file.readlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLIpc1V2XGCD"
      },
      "source": [
        "eng_fr = np.array([line.split(\"\\t\")[0:2] for line in lines])\r\n",
        "english = eng_fr[:, 0]\r\n",
        "french = eng_fr[:, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2O0Xhi2XaYl"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9X2SjCPXcRe"
      },
      "source": [
        "def punct(elt):\r\n",
        "  elts = re.sub(\"(?<=.)!\", \" !\", elt)\r\n",
        "  elts = re.sub(\"(?<=.)\\?\", \" ?\", elts)\r\n",
        "  elts = re.sub(\"(?<=.)\\.\", \" .\", elts)\r\n",
        "  elts = re.sub(\"(?<=.),\", \" ,\", elts)\r\n",
        "  elts = re.sub(\"(?<=.);\", \" ;\", elts)\r\n",
        "  return elts\r\n",
        "\r\n",
        "def start_end(elt):\r\n",
        "  return \"<start> \" + elt + \" <end>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4cfImi1XdhP"
      },
      "source": [
        "english_cleaned = [start_end(punct(elt)) for elt in english]\r\n",
        "french_cleaned = [start_end(punct(elt)) for elt in french]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHUBWELhXM_8"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BvCPE7p7XOv1"
      },
      "source": [
        "english_token = Tokenizer(filters='')\r\n",
        "english_token.fit_on_texts(english_cleaned)\r\n",
        "\r\n",
        "french_token = Tokenizer(filters='')\r\n",
        "french_token.fit_on_texts(french_cleaned)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cpIeBbtKXTDn"
      },
      "source": [
        "english_sentences = english_token.texts_to_sequences(english_cleaned)\r\n",
        "french_sentences = french_token.texts_to_sequences(french_cleaned)\r\n",
        "\r\n",
        "english_sentences = english_sentences[0:15000]\r\n",
        "french_sentences = french_sentences[0:15000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2NmiSOlXUkR"
      },
      "source": [
        "english_sentences = pad_sequences(english_sentences, padding='post')\r\n",
        "french_sentences = pad_sequences(french_sentences, padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tw1hRbaXlMr"
      },
      "source": [
        "### Tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShbpjscYXmGM"
      },
      "source": [
        "BUFFER_SIZE = len(english_sentences)\r\n",
        "BATCH_SIZE = 100\r\n",
        "vocab_english = len(english_token.word_index)+1\r\n",
        "vocab_french = len(french_token.word_index)+1\r\n",
        "\r\n",
        "dataset = tf.data.Dataset.from_tensor_slices((english_sentences, french_sentences)).shuffle(BUFFER_SIZE)\r\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhWEiSMTcIPU",
        "outputId": "be4f3403-b128-42d7-9ba5-b43fb563fed4"
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\r\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([100, 8]), TensorShape([100, 14]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xCSLKM7XtTv"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mF1SvVANXuN1"
      },
      "source": [
        "from tensorflow.keras.models import Model\r\n",
        "from tensorflow.keras.layers import Dropout, Dense, Embedding, LayerNormalization\r\n",
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeEz_t9L-jUQ"
      },
      "source": [
        "#### Masks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jof9DBcu-kiu"
      },
      "source": [
        "def look_ahead_mask(shape):\r\n",
        "  return 1 - tf.linalg.band_part(tf.ones(shape), -1, 0) \r\n",
        "\r\n",
        "def create_padding_mask(seq):\r\n",
        "  casted = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n",
        "  return casted[:, None, None, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDRqJzf-X7Mc"
      },
      "source": [
        "#### Scaled dot product multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRIjYVwHGNSp"
      },
      "source": [
        "def get_angles(pos, i, d_model):\r\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\r\n",
        "  return pos * angle_rates\r\n",
        "\r\n",
        "def positional_encoding(position, d_model):\r\n",
        "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n",
        "                          np.arange(d_model)[np.newaxis, :],\r\n",
        "                          d_model)\r\n",
        "\r\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n",
        "\r\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n",
        "\r\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\r\n",
        "\r\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3J98tWsBNl6"
      },
      "source": [
        "def attention(k, v, q, mask):\r\n",
        "\r\n",
        "  attention_scores = tf.linalg.matmul(q, k, transpose_b=True)\r\n",
        "  scaled_attention_scored = (attention_scores / k.shape[-1]) + (mask * -1e9)\r\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_scored, axis=-1)\r\n",
        "\r\n",
        "  output = tf.matmul(attention_weights, v)\r\n",
        "\r\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue4gsV_dYiSJ"
      },
      "source": [
        "class MultiHeadAttention(Layer):\r\n",
        "\r\n",
        "  def __init__(self, units, heads):\r\n",
        "    super(MultiHeadAttention, self).__init__()\r\n",
        "    self.v = Dense(units)\r\n",
        "    self.q = Dense(units)\r\n",
        "    self.k = Dense(units)\r\n",
        "    self.units = units\r\n",
        "    self.heads = heads\r\n",
        "    self.depth = self.units // self.heads\r\n",
        "\r\n",
        "  def split_head(self, data):\r\n",
        "    data = tf.reshape(data, (data.shape[0], -1, self.heads, self.depth))\r\n",
        "    return tf.transpose(data, perm=[0, 2, 1, 3])\r\n",
        "\r\n",
        "  def call(self, value, query, mask):\r\n",
        "\r\n",
        "    key_encoded = self.k(value)\r\n",
        "    query_encoded = self.q(query)\r\n",
        "    value_encoded = self.v(value)\r\n",
        "\r\n",
        "    key_encoded = self.split_head(key_encoded)\r\n",
        "    query_encoded = self.split_head(query_encoded)\r\n",
        "    value_encoded = self.split_head(value_encoded)\r\n",
        "    \r\n",
        "    output = attention(key_encoded, value_encoded, query_encoded, mask)\r\n",
        "    #reshape output\r\n",
        "    output = tf.transpose(output, perm=[0, 2, 1, 3])\r\n",
        "    output = tf.reshape(output, (output.shape[0], -1, self.units))\r\n",
        "\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YFA-nvB-XZH"
      },
      "source": [
        "#### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kj6BFgSVN_T"
      },
      "source": [
        "class Encoder_layer(Layer):\r\n",
        "  def __init__(self, units, num_heads):\r\n",
        "    super(Encoder_layer, self).__init__()\r\n",
        "    self.self_attention = MultiHeadAttention(units, num_heads)\r\n",
        "    self.norm_1 = LayerNormalization(epsilon=1e-6)\r\n",
        "    self.norm_2 = LayerNormalization(epsilon=1e-6)\r\n",
        "    self.feed_frwd_1 = Dense(100, activation=\"relu\")\r\n",
        "    self.feed_frwd_2 = Dense(units)\r\n",
        "    self.dropout_1 = Dropout(0.1)\r\n",
        "    self.dropout_2 = Dropout(0.1)\r\n",
        "\r\n",
        "\r\n",
        "  def call(self, data, encoder_pad_mask):\r\n",
        "\r\n",
        "    self_att_enc = self.self_attention(data, data, encoder_pad_mask)    \r\n",
        "\r\n",
        "    self_att_enc = self.dropout_2(self_att_enc)\r\n",
        "\r\n",
        "    normalised_out = self.norm_1(data + self_att_enc)\r\n",
        "\r\n",
        "    fc = self.feed_frwd_1(normalised_out)\r\n",
        "    fc2 = self.feed_frwd_2(fc)\r\n",
        "    fc2 = self.dropout_2(fc2)\r\n",
        "    output = self.norm_1(fc2 + normalised_out)\r\n",
        "\r\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9ZD9gPtWn6E"
      },
      "source": [
        "class Encoder(Layer):\r\n",
        "\r\n",
        "  def __init__(self, units, num_heads, vocab_size, embedding_dim, num_layers):\r\n",
        "    super(Encoder, self).__init__()\r\n",
        "    self.units = units\r\n",
        "    self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\r\n",
        "    self.embedding_dim = embedding_dim\r\n",
        "    self.num_layers = num_layers\r\n",
        "    self.encoders = [Encoder_layer(units, num_heads) for i in range(self.num_layers)]\r\n",
        "\r\n",
        "  def call(self, data, encoder_pad_mask):\r\n",
        "    data = self.embedding(data)\r\n",
        "    data = data + positional_encoding(data.shape[1], self.embedding_dim)\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      data = self.encoders[i](data, encoder_pad_mask)\r\n",
        "\r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o40B3x7VXsRw"
      },
      "source": [
        "enc_pad_mask = create_padding_mask(example_input_batch)\r\n",
        "encoder = Encoder(168, 8, vocab_english, 168, 4)\r\n",
        "encoded = encoder(example_input_batch, enc_pad_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXRH9TcK_CRG",
        "outputId": "3be0be7f-21db-43d3-eafe-342d25ddb3e3"
      },
      "source": [
        "encoded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 8, 168])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9f-Yiz2-cvr"
      },
      "source": [
        "#### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAGI-x6reVc0"
      },
      "source": [
        "class Decoder_layer(Layer):\r\n",
        "  def __init__(self, units, num_heads):\r\n",
        "    super(Decoder_layer, self).__init__()\r\n",
        "    self.self_attention_1 = MultiHeadAttention(units, num_heads)\r\n",
        "    self.self_attention_2 = MultiHeadAttention(units, num_heads)\r\n",
        "    self.norm_1 = LayerNormalization()\r\n",
        "    self.norm_2 = LayerNormalization()\r\n",
        "    self.norm_3 = LayerNormalization()\r\n",
        "    self.dropout_1 = Dropout(0.1)\r\n",
        "    self.dropout_2 = Dropout(0.1)\r\n",
        "    self.dropout_3 = Dropout(0.1)\r\n",
        "    self.feed_frwd_1 = Dense(100, activation=\"relu\")\r\n",
        "    self.feed_frwd_2 = Dense(units)\r\n",
        "\r\n",
        "  def call(self, target_input, encoder_input, combined_mask, padding_mask):\r\n",
        "\r\n",
        "    self_att_target = self.self_attention_1(target_input, target_input, combined_mask)\r\n",
        "    self_att_target = self.dropout_1(self_att_target)\r\n",
        "    normalised_out = self.norm_1(target_input + self_att_target)\r\n",
        "    \r\n",
        "    encoder_decoder_attention = self.self_attention_2(encoder_input, normalised_out, padding_mask)\r\n",
        "    encoder_decoder_attention = self.dropout_2(encoder_decoder_attention)\r\n",
        "    normalised_2_out = self.norm_2(encoder_decoder_attention + normalised_out)\r\n",
        "    \r\n",
        "    fc1 = self.feed_frwd_1(normalised_2_out)\r\n",
        "    fc2 = self.feed_frwd_2(fc1)\r\n",
        "    fc2 = self.dropout_3(fc2)\r\n",
        "\r\n",
        "    encoder_decoder_attention = self.norm_3(fc2 + normalised_2_out)\r\n",
        "\r\n",
        "    return encoder_decoder_attention\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgxc0AgWnllZ"
      },
      "source": [
        "class Decoder(Layer):\r\n",
        "\r\n",
        "  def __init__(self, units, num_heads, vocab_size, embedding_dim, num_layers):\r\n",
        "    super(Decoder, self).__init__()\r\n",
        "    self.units = units\r\n",
        "    self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\r\n",
        "    self.embedding_dim = embedding_dim\r\n",
        "    self.num_layers = num_layers\r\n",
        "    self.decoder = [Decoder_layer(units, num_heads) for i in range(self.num_layers)]\r\n",
        "\r\n",
        "  def call(self, data_encoded, data_target, combined_mask, pad_mask):\r\n",
        "    \r\n",
        "    data = self.embedding(data_target)\r\n",
        "    \r\n",
        "    data = data + positional_encoding(data.shape[1], self.embedding_dim)\r\n",
        "    \r\n",
        "    for i in range(self.num_layers):\r\n",
        "      data = self.decoder[i](data, data_encoded, combined_mask, pad_mask)\r\n",
        "    \r\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PIIVgdsg7cGF"
      },
      "source": [
        "dec_pad_mask = create_padding_mask(example_target_batch[:, :-1])\r\n",
        "l_h_mask = look_ahead_mask((13, 13))\r\n",
        "combined_mask = tf.math.maximum(dec_pad_mask, l_h_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MiiwC7AO4Cxz",
        "outputId": "4e6890e9-e577-4427-f82d-b2ed3b2aa58e"
      },
      "source": [
        "decoder = Decoder(168, 8, vocab_french, 168, 4)\r\n",
        "dec = decoder(encoded, example_target_batch[:, :-1], combined_mask, enc_pad_mask)\r\n",
        "dec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 13, 168])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CTNDuiB-fBR"
      },
      "source": [
        "#### Transformer class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhSa0-CEbPmS"
      },
      "source": [
        "class Transformer(Model):\r\n",
        "\r\n",
        "  def __init__(self, units, num_heads, inp_vocab, embedding_dim, num_layers, targ_vocab):\r\n",
        "    super(Transformer, self).__init__()\r\n",
        "    \r\n",
        "    self.encoder = Encoder(units, num_heads, inp_vocab, embedding_dim, num_layers)\r\n",
        "    self.decoder = Decoder(units, num_heads, targ_vocab, embedding_dim, num_layers)    \r\n",
        "    self.out = Dense(targ_vocab, activation=\"softmax\")\r\n",
        "    \r\n",
        "  def call(self, input_encoder, input_decoder, encoder_pad_mask, combined_mask):\r\n",
        "    \r\n",
        "    encoded_data = self.encoder(input_encoder, encoder_pad_mask)\r\n",
        "    decoded_data = self.decoder(encoded_data, input_decoder, combined_mask, encoder_pad_mask)\r\n",
        "    out = self.out(decoded_data)\r\n",
        "    \r\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAjZe6LFauV0"
      },
      "source": [
        "EMBEDDING_DIM = 168\r\n",
        "UNITS = 168\r\n",
        "NUM_LAYERS = 6\r\n",
        "NUM_HEADS = 8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BuhYmMZeWcH",
        "outputId": "7957fe25-dd68-44fc-90bf-288ddaa0078a"
      },
      "source": [
        "transformer = Transformer(UNITS, NUM_HEADS, vocab_english, EMBEDDING_DIM, NUM_LAYERS, vocab_french)\r\n",
        "transformed = transformer(example_input_batch, example_target_batch[:, :-1], enc_pad_mask, combined_mask)\r\n",
        "transformed.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 13, 34403])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9taNdG65ZjFB",
        "outputId": "2a360a12-1677-4059-9d9f-a9d7e7d15f80"
      },
      "source": [
        "combined_mask.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([100, 1, 13, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Zg7dFufrar"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKrYqe0d2XUu"
      },
      "source": [
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUzNtK84TheA"
      },
      "source": [
        "warmup_steps = 4000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCUf2jhk3AP"
      },
      "source": [
        "class Warmup_lr(LearningRateSchedule):\r\n",
        "\r\n",
        "  def __call__(self, step):\r\n",
        "    return (UNITS ** -0.5) * tf.math.minimum(step ** -0.5, step * (warmup_steps ** -1.5))\r\n",
        "\r\n",
        "learning_rate = Warmup_lr()\r\n",
        "\r\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EUubr6LMuoT"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\r\n",
        "    from_logits=True, reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fjmctjsfe8Y"
      },
      "source": [
        "def loss_fx(real, predicted):\r\n",
        "  categorical_loss = loss_object(real, predicted)\r\n",
        "  mask = tf.cast(\r\n",
        "      tf.math.not_equal(real, 0),\r\n",
        "      tf.float32\r\n",
        "  )\r\n",
        "  return tf.reduce_sum(categorical_loss * mask) / tf.reduce_sum(mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itNhw2a5ggtl"
      },
      "source": [
        "example_real = tf.stack([1,0,0])\r\n",
        "example_target = tf.stack([[0.,1.],[0.,1.],[1.,0.]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZdeKt5Wgyad",
        "outputId": "2c35154f-4c46-42d3-d648-c30300bde49f"
      },
      "source": [
        "loss_fx(example_real, example_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=0.31326166>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWdoh6eEZF8H"
      },
      "source": [
        "def train_step(input_text, target):\r\n",
        "  tar = target[:, 1:]\r\n",
        "  input_dec = target[:, :-1]\r\n",
        "\r\n",
        "  enc_pad_mask = create_padding_mask(input_text)\r\n",
        "  dec_pad_mask = create_padding_mask(input_text)\r\n",
        "\r\n",
        "  l_h_mask = look_ahead_mask((input_dec.shape[1], input_dec.shape[1]))\r\n",
        "  dec_target_padding_mask = create_padding_mask(input_dec)\r\n",
        "\r\n",
        "  combined_mask = tf.math.maximum(dec_target_padding_mask, l_h_mask)\r\n",
        "\r\n",
        "  with tf.GradientTape() as tape:\r\n",
        "    output = transformer(input_text, input_dec, enc_pad_mask, combined_mask)\r\n",
        "    loss = loss_fx(tar, output)\r\n",
        "\r\n",
        "  gradients = tape.gradient(loss, transformer.trainable_variables)\r\n",
        "  optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n",
        "\r\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iZOq8LYCiHk",
        "outputId": "215f0197-4794-4be6-da61-62bcfd368027"
      },
      "source": [
        "for epoch in range(50):\r\n",
        "  start = time.time()    \r\n",
        "  for batch, (inp, tar) in enumerate(dataset):\r\n",
        "    mean_loss = train_step(inp, tar)\r\n",
        "    if batch % 50 == 0:\r\n",
        "      print(\"batch {}, loss {}\".format(batch, mean_loss))\r\n",
        "  \r\n",
        "  print(\"Epoch {} time elapsed {} seconds\".format(epoch, time.time()-start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "batch 0, loss 10.431745529174805\n",
            "batch 50, loss 10.231319427490234\n",
            "batch 100, loss 9.832564353942871\n",
            "Epoch 0 time elapsed 56.67358183860779 seconds\n",
            "batch 0, loss 9.30780029296875\n",
            "batch 50, loss 8.635746955871582\n",
            "batch 100, loss 7.873263359069824\n",
            "Epoch 1 time elapsed 56.66216468811035 seconds\n",
            "batch 0, loss 7.034322738647461\n",
            "batch 50, loss 6.222867965698242\n",
            "batch 100, loss 5.588581562042236\n",
            "Epoch 2 time elapsed 56.23841905593872 seconds\n",
            "batch 0, loss 5.136653423309326\n",
            "batch 50, loss 4.826890468597412\n",
            "batch 100, loss 4.596340179443359\n",
            "Epoch 3 time elapsed 56.04957365989685 seconds\n",
            "batch 0, loss 4.375748634338379\n",
            "batch 50, loss 4.209961414337158\n",
            "batch 100, loss 4.079482078552246\n",
            "Epoch 4 time elapsed 56.05132746696472 seconds\n",
            "batch 0, loss 3.848883867263794\n",
            "batch 50, loss 3.802645206451416\n",
            "batch 100, loss 3.7814507484436035\n",
            "Epoch 5 time elapsed 55.793057680130005 seconds\n",
            "batch 0, loss 3.3991098403930664\n",
            "batch 50, loss 3.338036060333252\n",
            "batch 100, loss 3.4794554710388184\n",
            "Epoch 6 time elapsed 55.83855700492859 seconds\n",
            "batch 0, loss 3.0471384525299072\n",
            "batch 50, loss 3.063788652420044\n",
            "batch 100, loss 2.9658095836639404\n",
            "Epoch 7 time elapsed 55.730252504348755 seconds\n",
            "batch 0, loss 2.7433533668518066\n",
            "batch 50, loss 2.7502686977386475\n",
            "batch 100, loss 2.6350018978118896\n",
            "Epoch 8 time elapsed 55.36798429489136 seconds\n",
            "batch 0, loss 2.4298932552337646\n",
            "batch 50, loss 2.340884208679199\n",
            "batch 100, loss 2.520603895187378\n",
            "Epoch 9 time elapsed 55.29818916320801 seconds\n",
            "batch 0, loss 2.246325731277466\n",
            "batch 50, loss 2.193361282348633\n",
            "batch 100, loss 2.0912671089172363\n",
            "Epoch 10 time elapsed 55.32384657859802 seconds\n",
            "batch 0, loss 1.949284553527832\n",
            "batch 50, loss 2.029181957244873\n",
            "batch 100, loss 2.0880751609802246\n",
            "Epoch 11 time elapsed 55.42865991592407 seconds\n",
            "batch 0, loss 1.7996081113815308\n",
            "batch 50, loss 1.9940778017044067\n",
            "batch 100, loss 1.8264633417129517\n",
            "Epoch 12 time elapsed 55.639127254486084 seconds\n",
            "batch 0, loss 1.5924851894378662\n",
            "batch 50, loss 1.535237431526184\n",
            "batch 100, loss 1.5658506155014038\n",
            "Epoch 13 time elapsed 55.39951705932617 seconds\n",
            "batch 0, loss 1.2969590425491333\n",
            "batch 50, loss 1.4411135911941528\n",
            "batch 100, loss 1.2946430444717407\n",
            "Epoch 14 time elapsed 55.35224175453186 seconds\n",
            "batch 0, loss 1.13084077835083\n",
            "batch 50, loss 1.347530722618103\n",
            "batch 100, loss 1.1830798387527466\n",
            "Epoch 15 time elapsed 55.166191816329956 seconds\n",
            "batch 0, loss 1.0203412771224976\n",
            "batch 50, loss 1.1515376567840576\n",
            "batch 100, loss 1.1131622791290283\n",
            "Epoch 16 time elapsed 55.13121676445007 seconds\n",
            "batch 0, loss 0.8133825659751892\n",
            "batch 50, loss 0.9371245503425598\n",
            "batch 100, loss 0.9073888063430786\n",
            "Epoch 17 time elapsed 55.30760335922241 seconds\n",
            "batch 0, loss 0.7188903093338013\n",
            "batch 50, loss 0.8221303820610046\n",
            "batch 100, loss 0.7915201783180237\n",
            "Epoch 18 time elapsed 55.57944083213806 seconds\n",
            "batch 0, loss 0.5987505912780762\n",
            "batch 50, loss 0.6222184896469116\n",
            "batch 100, loss 0.7337769269943237\n",
            "Epoch 19 time elapsed 55.28152918815613 seconds\n",
            "batch 0, loss 0.6508339643478394\n",
            "batch 50, loss 0.5893049836158752\n",
            "batch 100, loss 0.6424089074134827\n",
            "Epoch 20 time elapsed 55.444830656051636 seconds\n",
            "batch 0, loss 0.5123299956321716\n",
            "batch 50, loss 0.5650346279144287\n",
            "batch 100, loss 0.6354270577430725\n",
            "Epoch 21 time elapsed 55.256656885147095 seconds\n",
            "batch 0, loss 0.4874304533004761\n",
            "batch 50, loss 0.5806804895401001\n",
            "batch 100, loss 0.6710405349731445\n",
            "Epoch 22 time elapsed 55.37963366508484 seconds\n",
            "batch 0, loss 0.46325501799583435\n",
            "batch 50, loss 0.5234448909759521\n",
            "batch 100, loss 0.6076933145523071\n",
            "Epoch 23 time elapsed 55.651792764663696 seconds\n",
            "batch 0, loss 0.4288618564605713\n",
            "batch 50, loss 0.5223464965820312\n",
            "batch 100, loss 0.5285264253616333\n",
            "Epoch 24 time elapsed 55.29763865470886 seconds\n",
            "batch 0, loss 0.46410414576530457\n",
            "batch 50, loss 0.5940438508987427\n",
            "batch 100, loss 0.5916294455528259\n",
            "Epoch 25 time elapsed 55.17442798614502 seconds\n",
            "batch 0, loss 0.4018518626689911\n",
            "batch 50, loss 0.42312854528427124\n",
            "batch 100, loss 0.4952068030834198\n",
            "Epoch 26 time elapsed 55.522369384765625 seconds\n",
            "batch 0, loss 0.44288840889930725\n",
            "batch 50, loss 0.7174767851829529\n",
            "batch 100, loss 0.5219776034355164\n",
            "Epoch 27 time elapsed 55.33336782455444 seconds\n",
            "batch 0, loss 0.41885676980018616\n",
            "batch 50, loss 0.3923625648021698\n",
            "batch 100, loss 0.535169243812561\n",
            "Epoch 28 time elapsed 55.246787309646606 seconds\n",
            "batch 0, loss 0.3795407712459564\n",
            "batch 50, loss 0.48215627670288086\n",
            "batch 100, loss 0.4727133512496948\n",
            "Epoch 29 time elapsed 55.528059005737305 seconds\n",
            "batch 0, loss 0.39047494530677795\n",
            "batch 50, loss 0.38277626037597656\n",
            "batch 100, loss 0.4428366422653198\n",
            "Epoch 30 time elapsed 55.17690062522888 seconds\n",
            "batch 0, loss 0.343374639749527\n",
            "batch 50, loss 0.3470499515533447\n",
            "batch 100, loss 0.43474990129470825\n",
            "Epoch 31 time elapsed 55.251912355422974 seconds\n",
            "batch 0, loss 0.3288458585739136\n",
            "batch 50, loss 0.35480526089668274\n",
            "batch 100, loss 0.38663405179977417\n",
            "Epoch 32 time elapsed 55.033689737319946 seconds\n",
            "batch 0, loss 0.3655136227607727\n",
            "batch 50, loss 0.3704451620578766\n",
            "batch 100, loss 0.4410225450992584\n",
            "Epoch 33 time elapsed 55.23632884025574 seconds\n",
            "batch 0, loss 0.3278980553150177\n",
            "batch 50, loss 0.3776918947696686\n",
            "batch 100, loss 0.46225982904434204\n",
            "Epoch 34 time elapsed 55.3194580078125 seconds\n",
            "batch 0, loss 0.2672213912010193\n",
            "batch 50, loss 0.33403080701828003\n",
            "batch 100, loss 0.3797306716442108\n",
            "Epoch 35 time elapsed 55.52407765388489 seconds\n",
            "batch 0, loss 0.3028548061847687\n",
            "batch 50, loss 0.37338191270828247\n",
            "batch 100, loss 0.3679129481315613\n",
            "Epoch 36 time elapsed 55.25723433494568 seconds\n",
            "batch 0, loss 0.31002676486968994\n",
            "batch 50, loss 0.31035077571868896\n",
            "batch 100, loss 0.4451431334018707\n",
            "Epoch 37 time elapsed 55.184326171875 seconds\n",
            "batch 0, loss 0.2802051901817322\n",
            "batch 50, loss 0.3346618413925171\n",
            "batch 100, loss 0.3496594727039337\n",
            "Epoch 38 time elapsed 55.116270542144775 seconds\n",
            "batch 0, loss 0.33349961042404175\n",
            "batch 50, loss 0.288979709148407\n",
            "batch 100, loss 0.3254009187221527\n",
            "Epoch 39 time elapsed 55.26452350616455 seconds\n",
            "batch 0, loss 0.27363309264183044\n",
            "batch 50, loss 0.3440033495426178\n",
            "batch 100, loss 0.37577927112579346\n",
            "Epoch 40 time elapsed 55.69226789474487 seconds\n",
            "batch 0, loss 0.28400373458862305\n",
            "batch 50, loss 0.3391283452510834\n",
            "batch 100, loss 0.35537877678871155\n",
            "Epoch 41 time elapsed 55.29706335067749 seconds\n",
            "batch 0, loss 0.3118603825569153\n",
            "batch 50, loss 0.3261452317237854\n",
            "batch 100, loss 0.30320867896080017\n",
            "Epoch 42 time elapsed 55.377567291259766 seconds\n",
            "batch 0, loss 0.28832995891571045\n",
            "batch 50, loss 0.35425087809562683\n",
            "batch 100, loss 0.3327186107635498\n",
            "Epoch 43 time elapsed 55.089295387268066 seconds\n",
            "batch 0, loss 0.27010074257850647\n",
            "batch 50, loss 0.3067595660686493\n",
            "batch 100, loss 0.36652708053588867\n",
            "Epoch 44 time elapsed 54.98842239379883 seconds\n",
            "batch 0, loss 0.2489563375711441\n",
            "batch 50, loss 0.3167896866798401\n",
            "batch 100, loss 0.3325812816619873\n",
            "Epoch 45 time elapsed 55.16721534729004 seconds\n",
            "batch 0, loss 0.2524693012237549\n",
            "batch 50, loss 0.34689921140670776\n",
            "batch 100, loss 0.3396891951560974\n",
            "Epoch 46 time elapsed 55.239136934280396 seconds\n",
            "batch 0, loss 0.23654994368553162\n",
            "batch 50, loss 0.3125551640987396\n",
            "batch 100, loss 0.29208579659461975\n",
            "Epoch 47 time elapsed 54.950140714645386 seconds\n",
            "batch 0, loss 0.26415690779685974\n",
            "batch 50, loss 0.3433622717857361\n",
            "batch 100, loss 0.3558657169342041\n",
            "Epoch 48 time elapsed 55.09092736244202 seconds\n",
            "batch 0, loss 0.2299540638923645\n",
            "batch 50, loss 0.21378380060195923\n",
            "batch 100, loss 0.31498482823371887\n",
            "Epoch 49 time elapsed 55.687342166900635 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwVEl3ObOi9C"
      },
      "source": [
        "### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eo5_IQPyXZ1z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f78f0a3e-b79f-4d24-cdb2-190871d6dc54"
      },
      "source": [
        "example_input_batch[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([   1,  212,    9, 2922,    3,    2,    0,    0], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2BsLuZmR3h_"
      },
      "source": [
        "# Masks : \r\n",
        "\r\n",
        "def translate(sentence):\r\n",
        "    \r\n",
        "  test = [\"<start>\" + \" \" + sentence + \" \" + \"<end>\"]\r\n",
        "  tokenized_sent = tf.convert_to_tensor(english_token.texts_to_sequences(test))\r\n",
        "\r\n",
        "  max_size = 30\r\n",
        "  output = tf.convert_to_tensor([1])\r\n",
        "  output = tf.expand_dims(output, 0)\r\n",
        "\r\n",
        "\r\n",
        "  for i in range(15):\r\n",
        "    enc_pad_mask = create_padding_mask(tokenized_sent)\r\n",
        "    dec_pad_mask = create_padding_mask(tokenized_sent)\r\n",
        "\r\n",
        "    l_h_mask = look_ahead_mask((output.shape[1], output.shape[1]))\r\n",
        "    dec_target_padding_mask = create_padding_mask(output)\r\n",
        "\r\n",
        "    combined_mask = tf.math.maximum(dec_target_padding_mask, l_h_mask)\r\n",
        "\r\n",
        "    prediction = transformer(tokenized_sent, output, enc_pad_mask, combined_mask)\r\n",
        "\r\n",
        "    id_pred = tf.argmax(prediction[..., -1:, :], axis=-1, output_type=tf.int32)\r\n",
        "    output = tf.concat([output, id_pred], axis=-1)\r\n",
        "    \r\n",
        "  return french_token.sequences_to_texts(output.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t4dpBqTJegT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078c42d4-a9c8-4098-a812-942f7d8944e5"
      },
      "source": [
        "translate(\"I am pretty\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<start> je suis jolie . <end> <end> <end> . <end> <end> <end> <end> <end> <end> <end>']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    }
  ]
}